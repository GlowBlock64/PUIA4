{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9423d-0386-4649-b179-c2362c9ded0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         parent_text  label\n",
      "0  Ah, then apologies for my ignorance, and thank...      0\n",
      "1  Given the current state of our relations with ...      1\n",
      "2  From my experience, doctors are generally unde...      1\n",
      "3                       It will be too late by then.      1\n",
      "4  Sadly this story has died down why is he bring...      0\n",
      "Saved to parent_child_labels.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = \"GoEmotions/data/relabeled_dataset.csv\"\n",
    "\n",
    "df = pd.read_csv(path, usecols=[\"text\", \"id\", \"author\", \"subreddit\", \"parent_id\", \"created_utc\", \"label\"])\n",
    "df['parent_id'] = df['parent_id'].str[3:]\n",
    "\n",
    "merged = df.merge(df, left_on='parent_id', right_on='id', suffixes=('_child', '_parent'))\n",
    "\n",
    "result = pd.DataFrame({\n",
    "    'parent_text': merged['text_parent'],\n",
    "    'label': (merged['label_child'] == 3).astype(int)\n",
    "})\n",
    "\n",
    "parents_only = df[df['label'].isin([1, 3])]\n",
    "\n",
    "parents_only_labels = (parents_only['label'] == 3).astype(int)\n",
    "\n",
    "parents_only = parents_only.assign(\n",
    "    label=parents_only_labels,\n",
    "    rand=np.random.rand(len(parents_only))\n",
    ")\n",
    "\n",
    "parents_sampled = parents_only[parents_only['rand'] < 0.1]\n",
    "\n",
    "parents_df = pd.DataFrame({\n",
    "    'parent_text': parents_sampled['text'],\n",
    "    'label': parents_sampled['label']\n",
    "})\n",
    "\n",
    "result = pd.concat([result, parents_df], ignore_index=True)\n",
    "\n",
    "print(result.head())\n",
    "result.to_csv(\"parent_child_labels.csv\", index=False)\n",
    "print(\"Saved to parent_child_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f0599-08c4-4f76-95a1-24cde7d3ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4070' max='4070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4070/4070 03:17, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.647854</td>\n",
       "      <td>0.605166</td>\n",
       "      <td>0.751740</td>\n",
       "      <td>0.602230</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.578600</td>\n",
       "      <td>0.573150</td>\n",
       "      <td>0.731857</td>\n",
       "      <td>0.798893</td>\n",
       "      <td>0.724080</td>\n",
       "      <td>0.890947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.530524</td>\n",
       "      <td>0.745387</td>\n",
       "      <td>0.797258</td>\n",
       "      <td>0.760748</td>\n",
       "      <td>0.837449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.547700</td>\n",
       "      <td>0.514421</td>\n",
       "      <td>0.751538</td>\n",
       "      <td>0.793456</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.798354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.417300</td>\n",
       "      <td>0.509477</td>\n",
       "      <td>0.753998</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.801688</td>\n",
       "      <td>0.781893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.602900</td>\n",
       "      <td>0.506698</td>\n",
       "      <td>0.762608</td>\n",
       "      <td>0.803662</td>\n",
       "      <td>0.794769</td>\n",
       "      <td>0.812757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.395200</td>\n",
       "      <td>0.507856</td>\n",
       "      <td>0.758918</td>\n",
       "      <td>0.797938</td>\n",
       "      <td>0.799587</td>\n",
       "      <td>0.796296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.442700</td>\n",
       "      <td>0.507045</td>\n",
       "      <td>0.765068</td>\n",
       "      <td>0.806091</td>\n",
       "      <td>0.795591</td>\n",
       "      <td>0.816872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.507734</td>\n",
       "      <td>0.762608</td>\n",
       "      <td>0.803262</td>\n",
       "      <td>0.795960</td>\n",
       "      <td>0.810700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.409400</td>\n",
       "      <td>0.508069</td>\n",
       "      <td>0.765068</td>\n",
       "      <td>0.805697</td>\n",
       "      <td>0.796781</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./parent_model/tokenizer_config.json',\n",
       " './parent_model/special_tokens_map.json',\n",
       " './parent_model/vocab.txt',\n",
       " './parent_model/added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "class ParentDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "texts = result['parent_text'].tolist()\n",
    "labels = result['label'].tolist()\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = ParentDataset(train_texts, train_labels)\n",
    "val_dataset = ParentDataset(val_texts, val_labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    prec = precision_score(labels, preds)\n",
    "    rec = recall_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': prec,\n",
    "        'recall': rec\n",
    "    }\n",
    "\n",
    "# Training config with eval\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.1,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=1e-6,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model('./parent_model')\n",
    "tokenizer.save_pretrained('./parent_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f5428-84ec-428a-8bf0-da743fe9e32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
